{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732c08e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     ModalityType\u001b[38;5;241m.\u001b[39mTEXT: data\u001b[38;5;241m.\u001b[39mload_and_transform_text(text_list, device),\n\u001b[0;32m---> 21\u001b[0m     ModalityType\u001b[38;5;241m.\u001b[39mVISION: \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_transform_vision_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     22\u001b[0m     ModalityType\u001b[38;5;241m.\u001b[39mAUDIO: data\u001b[38;5;241m.\u001b[39mload_and_transform_audio_data(audio_paths, device),\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     26\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind/lib/python3.10/site-packages/imagebind/data.py:105\u001b[0m, in \u001b[0;36mload_and_transform_vision_data\u001b[0;34m(image_paths, device)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fopen:\n\u001b[1;32m    103\u001b[0m         image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(fopen)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mdata_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    106\u001b[0m     image_outputs\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(image_outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind/lib/python3.10/site-packages/torchvision/transforms/functional.py:163\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    162\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 163\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "#test imagebind functioality\n",
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\"/ImageBind/.assets/dog_image.jpg\", \"/ImageBind/.assets/car_image.jpg\", \"/ImageBind/.assets/bird_image.jpg\"]\n",
    "audio_paths=[\"/ImageBind/.assets/dog_audio.wav\", \"/ImageBind/.assets/car_audio.wav\", \"/ImageBind/.assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6f81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>2 in /root/miniconda3/envs/imagebind/lib/python3.10/site-packages (2.2.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy>2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c9d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2                                           \u001b[0m\u001b[01;36mnvidia-examples\u001b[0m@\n",
      "NVIDIA_Deep_Learning_Container_License.pdf  \u001b[01;34mproj\u001b[0m/\n",
      "README.md                                   \u001b[01;34msolanity\u001b[0m/\n",
      "\u001b[01;34mchroma_db\u001b[0m/                                  \u001b[01;34msolanity-with-suffix\u001b[0m/\n",
      "\u001b[01;34mdocker-examples\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a3f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting optimized Arweave webpage indexing...\n",
      "🚀 Starting optimized Arweave indexing pipeline...\n",
      "📄 Processing webpage 1: http://arweave.net/3sAdgGV5efxdqxQOTtH_6hHtRmmY1iyywUb3aB0EHx8\n",
      "📝 Chunk 1: Permaweb LLM Fuel...\n",
      "✅ Processed 1 webpages, created 1 valid chunks (total chunks: 1, skipped: 0)\n",
      "📄 Processing webpage 2: http://arweave.net/rRQZ8p4V83BweFtOMfXLJ6xMfc_81-AyKhqkbZ-cmu0\n",
      "⚠️  Skipped http://arweave.net/rRQZ8p4V83BweFtOMfXLJ6xMfc_81-AyKhqkbZ-cmu0 - invalid content\n",
      "📄 Processing webpage 2: http://arweave.net/JMEUEcnwNQCpt9Cd5z9Oy3xMQ9cst_hXkT-4-4YvQM0\n",
      "⚠️  Skipped http://arweave.net/JMEUEcnwNQCpt9Cd5z9Oy3xMQ9cst_hXkT-4-4YvQM0 - invalid content\n",
      "📄 Processing webpage 2: http://arweave.net/zFGZ14V1JYPmlAyn2p054tm2F8iWYIBqkmrecqmS4Xw\n",
      "⚠️  Skipped http://arweave.net/zFGZ14V1JYPmlAyn2p054tm2F8iWYIBqkmrecqmS4Xw - invalid content\n",
      "📄 Processing webpage 2: http://arweave.net/_rDg7_NmAmF4j-mvLap81RBLVCBAFLBGLh_o8yN8cFA\n",
      "📝 Chunk 2: Permaweb Memes...\n",
      "✅ Processed 2 webpages, created 1 valid chunks (total chunks: 2, skipped: 3)\n",
      "📄 Processing webpage 3: http://arweave.net/bZ9PwcSb1JSj-q7HLMpRi6v5Jz2NoWvwOUX1Kx1_2eA\n",
      "📝 Chunk 3: My ceasefire conditions - Federal Document...\n",
      "✅ Processed 3 webpages, created 1 valid chunks (total chunks: 3, skipped: 3)\n",
      "📄 Processing webpage 4: http://arweave.net/lmJXLT_vZ1Qy8lWxb73W32gqyU2S3BwBXhlUKNM0FzI\n",
      "📝 Chunk 4: Anon's AO Signature dApp...\n",
      "✅ Processed 4 webpages, created 1 valid chunks (total chunks: 4, skipped: 3)\n",
      "📄 Processing webpage 5: http://arweave.net/MQmLQ8Kh48rBNuD9fkLUcZu4zz9F-Y6Z780PRikAx-g\n",
      "⚠️  Skipped http://arweave.net/MQmLQ8Kh48rBNuD9fkLUcZu4zz9F-Y6Z780PRikAx-g - invalid content\n",
      "📄 Processing webpage 5: http://arweave.net/y_-4m5EdSHVCiH9yE9p3uiWkxsm4kKsOOC2GLn-E0l8\n",
      "⚠️  Skipped http://arweave.net/y_-4m5EdSHVCiH9yE9p3uiWkxsm4kKsOOC2GLn-E0l8 - invalid content\n",
      "📄 Processing webpage 5: http://arweave.net/Ls0K9EHixFutJpKiOoyQKd37yKA6EmIMChQ-7WTC9Vg\n",
      "📝 Chunk 5: Anon's AO Signature dApp...\n",
      "✅ Processed 5 webpages, created 1 valid chunks (total chunks: 5, skipped: 5)\n",
      "📄 Processing webpage 6: http://arweave.net/GiFCP7SDpr6LNiOmVfpTTmp483K16cozZpglO4bwxeU\n",
      "📝 Chunk 6: Local Arseeding Deployment...\n",
      "✅ Processed 6 webpages, created 1 valid chunks (total chunks: 6, skipped: 5)\n",
      "📄 Processing webpage 7: http://arweave.net/ZzuA-ym8spp8h0rEK0FptvRFlP1g5ejv0nhIjceHCIA\n",
      "📝 Chunk 7: Zero-to-Arweave Starter Kit...\n",
      "✅ Processed 7 webpages, created 1 valid chunks (total chunks: 7, skipped: 5)\n",
      "📄 Processing webpage 8: http://arweave.net/Ma0dLIFI3xGJgAgKE2VC0uoe1f0mGIuhLXyaid7QEOY\n",
      "📝 Chunk 8: Arseeding (Web3Infra) Deployment...\n",
      "✅ Processed 8 webpages, created 1 valid chunks (total chunks: 8, skipped: 5)\n",
      "📄 Processing webpage 9: http://arweave.net/phw6hdwkg1Hc_vogC8ZpuBVRdSl-6YxtZ9CtZw0XCTo\n",
      "📝 Chunk 9: SECRET FUN - Anon's AO dApp - Part 1/2...\n",
      "📝 Chunk 10: SECRET FUN - Anon's AO dApp - Part 2/2...\n",
      "✅ Processed 9 webpages, created 2 valid chunks (total chunks: 10, skipped: 5)\n",
      "📄 Processing webpage 10: http://arweave.net/ZSalRc9ln_4tXlFW9ilbUU1RU3OOt32fdLDzHz3fe1Y\n",
      "📝 Chunk 11: pop upcOS hasta que terminemos...\n",
      "✅ Processed 10 webpages, created 1 valid chunks (total chunks: 11, skipped: 5)\n",
      "🎉 Indexing complete! Processed 10 webpages, created 11 chunks, skipped 5 items\n",
      "🎉 Indexed 11 valid chunks from Arweave webpages!\n",
      "\n",
      "📋 Sample chunk:\n",
      "Title: Permaweb LLM Fuel\n",
      "URL: http://arweave.net/3sAdgGV5efxdqxQOTtH_6hHtRmmY1iyywUb3aB0EHx8\n",
      "Content length: 340 chars\n",
      "Content preview: Permaweb LLM Fuel                       Permaweb LLM Fuel \n",
      "All these docs have been collected throughout the permaweb. Feel free to select which ones to fuel your LLM.\n",
      "      Loading documentation inde...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "def is_valid_content(content):\n",
    "    \"\"\"\n",
    "    Check if content is valid for indexing.\n",
    "    Returns True if content is meaningful, False otherwise.\n",
    "    \"\"\"\n",
    "    if not content:\n",
    "        return False\n",
    "    \n",
    "    # Remove whitespace and check if empty\n",
    "    stripped = content.strip()\n",
    "    if not stripped:\n",
    "        return False\n",
    "    \n",
    "    # Check if content is too short (likely not meaningful)\n",
    "    if len(stripped) < 50:\n",
    "        return False\n",
    "    \n",
    "    # Check if content is mostly whitespace or special characters\n",
    "    text_ratio = len(re.findall(r'[a-zA-Z0-9]', stripped)) / len(stripped)\n",
    "    if text_ratio < 0.3:  # Less than 30% actual text\n",
    "        return False\n",
    "    \n",
    "    # Check for common meaningless patterns\n",
    "    meaningless_patterns = [\n",
    "        r'^\\s*$',  # Only whitespace\n",
    "        r'^[^\\w]*$',  # Only special characters\n",
    "        r'^(Loading|Error|404|Not Found|Access Denied)',  # Error pages\n",
    "        r'^\\s*(javascript|css|html)\\s*$',  # Just tech terms\n",
    "    ]\n",
    "    \n",
    "    for pattern in meaningless_patterns:\n",
    "        if re.match(pattern, stripped, re.IGNORECASE):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_webpage(url):\n",
    "    \"\"\"\n",
    "    Checks if the given URL points to a webpage (HTML content).\n",
    "    Returns True if the content appears to be a webpage, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        text = resp.text\n",
    "        if \"<!doctype html\" in text.lower() or \"<html\" in text.lower():\n",
    "            return True\n",
    "        if any(tag in text.lower() for tag in [\"<head\", \"<body\", \"<title\", \"<meta\"]):\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_arweave_webpage_manifests(max_workers=16, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generator function that yields Arweave manifest transaction IDs\n",
    "    that actually point to webpages (HTML content).\n",
    "    Yields: tuple of (url, content_type)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    query GetManifestTxs($cursor: String) {\n",
    "      transactions(\n",
    "        after: $cursor\n",
    "        first: 100\n",
    "        tags: [\n",
    "          { name: \"Content-Type\", values: [\"application/x.arweave-manifest+json\"] }\n",
    "        ]\n",
    "      ) {\n",
    "        pageInfo {\n",
    "          hasNextPage\n",
    "        }\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            id\n",
    "            tags {\n",
    "              name\n",
    "              value\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://arweave.net/graphql\"\n",
    "    headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "    cursor = None\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"variables\": { \"cursor\": cursor } if cursor else {}\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching or decoding response: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            edges = result[\"data\"][\"transactions\"][\"edges\"]\n",
    "            page_info = result[\"data\"][\"transactions\"][\"pageInfo\"]\n",
    "        except (KeyError, TypeError):\n",
    "            print(\"Malformed response or missing data in GraphQL result.\")\n",
    "            break\n",
    "\n",
    "        if not edges:\n",
    "            break\n",
    "\n",
    "        txs = []\n",
    "        for edge in edges:\n",
    "            tx = edge[\"node\"]\n",
    "            txid = tx[\"id\"]\n",
    "            content_type = None\n",
    "            for tag in tx.get(\"tags\", []):\n",
    "                if tag.get(\"name\") == \"Content-Type\":\n",
    "                    content_type = tag.get(\"value\")\n",
    "                    break\n",
    "            if content_type:\n",
    "                txs.append((txid, content_type))\n",
    "\n",
    "        for i in range(0, len(txs), batch_size):\n",
    "            batch = txs[i:i+batch_size]\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                future_to_tx = {\n",
    "                    executor.submit(check_webpage, f\"http://arweave.net/{txid}\"): (txid, content_type)\n",
    "                    for txid, content_type in batch\n",
    "                }\n",
    "                for future in as_completed(future_to_tx):\n",
    "                    txid, content_type = future_to_tx[future]\n",
    "                    try:\n",
    "                        is_webpage = future.result()\n",
    "                    except Exception:\n",
    "                        is_webpage = False\n",
    "                    if is_webpage:\n",
    "                        yield f\"http://arweave.net/{txid}\", content_type\n",
    "\n",
    "        count += len(edges)\n",
    "        print(f\"✅ {count} manifest txs indexed...\")\n",
    "\n",
    "        if not page_info.get(\"hasNextPage\"):\n",
    "            break\n",
    "\n",
    "        cursor = edges[-1].get(\"cursor\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def load_webpage_content(url):\n",
    "    \"\"\"\n",
    "    Load webpage content using LangChain WebBaseLoader.\n",
    "    Returns Document object with content and metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        if docs:\n",
    "            return docs[0]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_chunks(doc, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split document into chunks for embedding.\n",
    "    Returns list of Document chunks with metadata.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Get title for chunk descriptions\n",
    "    title = doc.metadata.get(\"title\", \"Untitled\")\n",
    "    \n",
    "    # Add chunk-specific metadata and descriptions\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Create individual chunk description\n",
    "        chunk_desc = create_chunk_description(\n",
    "            chunk.page_content, \n",
    "            i, \n",
    "            len(chunks), \n",
    "            title\n",
    "        )\n",
    "        \n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"chunk_size\": len(chunk.page_content),\n",
    "            \"content_hash\": hashlib.md5(chunk.page_content.encode()).hexdigest(),\n",
    "            \"chunk_description\": chunk_desc,\n",
    "            \"chunk_title\": f\"{title} - Part {i + 1}/{len(chunks)}\" if len(chunks) > 1 else title\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_metadata(doc):\n",
    "    \"\"\"\n",
    "    Extract rich metadata from document.\n",
    "    Returns enhanced metadata dictionary.\n",
    "    \"\"\"\n",
    "    metadata = doc.metadata.copy()\n",
    "    \n",
    "    # Extract title and description\n",
    "    title = metadata.get(\"title\", \"Untitled\")\n",
    "    description = metadata.get(\"description\", \"\")\n",
    "    \n",
    "    # Create a short description from content\n",
    "    content = doc.page_content\n",
    "    if len(content) > 200:\n",
    "        short_desc = content[:200] + \"...\"\n",
    "    else:\n",
    "        short_desc = content\n",
    "    \n",
    "    # Enhanced metadata\n",
    "    enhanced_metadata = {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"short_description\": short_desc,\n",
    "        \"url\": metadata.get(\"source\", \"\"),\n",
    "        \"content_length\": len(content),\n",
    "        \"language\": \"en\",  # Could be detected later\n",
    "        \"content_type\": \"webpage\",\n",
    "        \"source\": \"arweave\",\n",
    "        \"extracted_at\": time.time()\n",
    "    }\n",
    "    \n",
    "    return enhanced_metadata\n",
    "\n",
    "def create_chunk_description(chunk_content, chunk_id, total_chunks, title):\n",
    "    \"\"\"\n",
    "    Create a meaningful short description for each chunk.\n",
    "    \"\"\"\n",
    "    # Clean the chunk content\n",
    "    cleaned_content = chunk_content.strip()\n",
    "    \n",
    "    # Create a short description from the chunk content\n",
    "    if len(cleaned_content) > 150:\n",
    "        chunk_desc = cleaned_content[:150] + \"...\"\n",
    "    else:\n",
    "        chunk_desc = cleaned_content\n",
    "    \n",
    "    # Add chunk context to the description\n",
    "    if total_chunks > 1:\n",
    "        chunk_desc = f\"[Part {chunk_id + 1}/{total_chunks}] {chunk_desc}\"\n",
    "    \n",
    "    # Add title context if available\n",
    "    if title and title != \"Untitled\":\n",
    "        chunk_desc = f\"{title}: {chunk_desc}\"\n",
    "    \n",
    "    return chunk_desc\n",
    "\n",
    "def optimized_arweave_indexer(max_pages=None, chunk_size=1000, chunk_overlap=200, max_workers=16, batch_size=32):\n",
    "    \"\"\"\n",
    "    Super optimized generator function that does everything in one pipeline:\n",
    "    - Discovers Arweave webpages\n",
    "    - Validates content quality\n",
    "    - Extracts and chunks content\n",
    "    - Yields ready-to-embed documents\n",
    "    \n",
    "    Args:\n",
    "        max_pages: Maximum number of pages to process\n",
    "        chunk_size: Size of each chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        max_workers: Number of parallel workers for webpage checking\n",
    "        batch_size: Batch size for parallel processing\n",
    "    \n",
    "    Yields:\n",
    "        Document objects ready for embedding with rich metadata\n",
    "    \"\"\"\n",
    "    page_count = 0\n",
    "    chunk_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    print(\"🚀 Starting optimized Arweave indexing pipeline...\")\n",
    "    \n",
    "    for url, content_type in get_arweave_webpage_manifests(max_workers, batch_size):\n",
    "        if max_pages and page_count >= max_pages:\n",
    "            break\n",
    "            \n",
    "        print(f\"📄 Processing webpage {page_count + 1}: {url}\")\n",
    "        \n",
    "        # Load webpage content\n",
    "        doc = load_webpage_content(url)\n",
    "        if not doc:\n",
    "            skipped_count += 1\n",
    "            print(f\"⚠️  Skipped {url} - failed to load\")\n",
    "            continue\n",
    "        \n",
    "        # Validate content quality\n",
    "        if not is_valid_content(doc.page_content):\n",
    "            skipped_count += 1\n",
    "            print(f\"⚠️  Skipped {url} - invalid content\")\n",
    "            continue\n",
    "        \n",
    "        # Extract rich metadata\n",
    "        enhanced_metadata = extract_metadata(doc)\n",
    "        doc.metadata.update(enhanced_metadata)\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = create_chunks(doc, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Filter out invalid chunks and yield valid ones\n",
    "        valid_chunks = 0\n",
    "        for chunk in chunks:\n",
    "            if is_valid_content(chunk.page_content):\n",
    "                # Update with enhanced metadata but preserve chunk-specific descriptions\n",
    "                chunk.metadata.update(enhanced_metadata)\n",
    "                # Keep the chunk-specific description and title\n",
    "                chunk.metadata[\"short_description\"] = chunk.metadata.get(\"chunk_description\", chunk.metadata.get(\"short_description\", \"\"))\n",
    "                chunk.metadata[\"title\"] = chunk.metadata.get(\"chunk_title\", chunk.metadata.get(\"title\", \"Untitled\"))\n",
    "                \n",
    "                chunk_count += 1\n",
    "                valid_chunks += 1\n",
    "                yield chunk\n",
    "        \n",
    "        page_count += 1\n",
    "        print(f\"✅ Processed {page_count} webpages, created {valid_chunks} valid chunks (total chunks: {chunk_count}, skipped: {skipped_count})\")\n",
    "    \n",
    "    print(f\"🎉 Indexing complete! Processed {page_count} webpages, created {chunk_count} chunks, skipped {skipped_count} items\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔍 Starting optimized Arweave webpage indexing...\")\n",
    "    \n",
    "    # Process first 10 webpages as example\n",
    "    chunks = []\n",
    "    for chunk in optimized_arweave_indexer(max_pages=10):\n",
    "        chunks.append(chunk)\n",
    "        print(f\"📝 Chunk {len(chunks)}: {chunk.metadata['title'][:50]}...\")\n",
    "    \n",
    "    print(f\"🎉 Indexed {len(chunks)} valid chunks from Arweave webpages!\")\n",
    "    \n",
    "    # Example: Show first chunk details\n",
    "    if chunks:\n",
    "        first_chunk = chunks[0]\n",
    "        print(f\"\\n📋 Sample chunk:\")\n",
    "        print(f\"Title: {first_chunk.metadata['title']}\")\n",
    "        print(f\"URL: {first_chunk.metadata['url']}\")\n",
    "        print(f\"Content length: {len(first_chunk.page_content)} chars\")\n",
    "        print(f\"Content preview: {first_chunk.page_content[:200]}...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71456d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'arweave', 'title': 'pop upcOS hasta que terminemos', 'language': 'en', 'description': '', 'short_description': 'pop upcOS hasta que terminemos: pop upcOS hasta que terminemosYou need to enable JavaScript to run this app.', 'url': 'http://arweave.net/ZSalRc9ln_4tXlFW9ilbUU1RU3OOt32fdLDzHz3fe1Y', 'content_length': 76, 'content_type': 'webpage', 'extracted_at': 1754148966.0766518, 'chunk_id': 0, 'total_chunks': 1, 'chunk_size': 76, 'content_hash': '0d6297e570c2bd8703147112f9b28b25', 'chunk_description': 'pop upcOS hasta que terminemos: pop upcOS hasta que terminemosYou need to enable JavaScript to run this app.', 'chunk_title': 'pop upcOS hasta que terminemos'}, page_content='pop upcOS hasta que terminemosYou need to enable JavaScript to run this app.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feeef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgbind",
   "language": "python",
   "name": "imgbind"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
